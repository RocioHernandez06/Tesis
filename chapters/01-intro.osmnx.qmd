---
title: "Redes LSTM ( Long Short-Term Memory): Memoria a Largo y Corto Plazo"
author: "Presentaci√≥n sobre Redes Neuronales Recurrentes"
date: "Octubre 2025"
format:
  beamer:
    keep-tex: true
    slide-level: 2
    theme: "Madrid"
    colortheme: "dolphin"
  html:
    theme: "cosmo"
    toc: true
    toc-depth: 2
    css: styles.css
---

## ¬øQu√© son las LSTM?

- **Extensi√≥n de las RNN** dise√±adas para capturar dependencias temporales de secuencias largas
- Las RNN b√°sicas **luchan por capturar** dependencias largas en el tiempo
- Introducen el concepto de **"celda de memoria"** que puede mantener, escribir o leer informaci√≥n
- Controladas por **estructuras llamadas puertas** que gestionan el flujo de informaci√≥n

## Problemas que Resuelven las LSTM

### Limitaciones de las RNN B√°sicas
- **Problema del gradiente de fuga**
- Dificultad para considerar **secuencias de entrada largas**
- **P√©rdida de contexto** en dependencias temporales largas

### Soluciones de las LSTM
- Las alteraciones en los LSTM **abordan el problema del gradiente de fuga**
- Permiten considerar **secuencias de entrada mucho m√°s largas**
- **Preservan el contexto** de manera m√°s efectiva

## Arquitectura LSTM: Las Tres Puertas

### Componentes Fundamentales
1. **Puerta de entrada** - Decide qu√© valores son importantes
2. **Puerta de olvido** - Descarta informaci√≥n innecesaria  
3. **Puerta de salida** - Decide qu√© informaci√≥n pasar al siguiente paso

### Caracter√≠sticas Comunes
- Consideran las **entradas del paso de tiempo anterior**
- Modifican la **memoria del modelo** y los **pesos de entrada**
- Utilizan **funciones de activaci√≥n espec√≠ficas** para cada puerta

![Arquitectura LSTM](LSTM.png){width=50%}

## Puerta de Olvido

### Funci√≥n Principal
Descarta **informaci√≥n que el modelo considera innecesaria** para tomar decisiones.
Ecuaci√≥n Matem√°tica
$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

### Componentes
- $W_f$: **peso de la puerta del olvido**
- $b_f$: **sesgo de la puerta de olvido**
- $\sigma$: **funci√≥n sigmoidea** (valores 0-1)

### Interpretaci√≥n
- **0**: Olvidar completamente
- **1**: Retener completamente
- Determina qu√© informaci√≥n del estado anterior conservar

## Puerta de Entrada

### Funci√≥n Principal
Decide **qu√© valores son importantes** y deben transmitirse por el modelo.

### Ecuaciones
**Decidir qu√© actualizar:**
$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$

**Generar valores candidatos:**
$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

### Funciones de Activaci√≥n
- **Sigmoidea**: decide qu√© valores transmitir (0 = reducir, 1 = conservar)
- **TanH**: decide importancia de valores (-1 a 1)

## C√©lula de Memoria Candidata

### Funci√≥n
Genera **nueva informaci√≥n potencial** para almacenar en el estado de la celda.

### Ecuaci√≥n
$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

### Caracter√≠sticas
- Genera **valores candidatos** para agregar al estado
- Funci√≥n **tanh** asegura valores entre -1 y 1
- Representa **informaci√≥n potencial** para almacenar

## Actualizaci√≥n del Estado de la Celda

### Proceso de Actualizaci√≥n
$$C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t$$

### Explicaci√≥n del Proceso
1. Estado anterior ($C_{t-1}$) √ó Puerta de olvido ($f_t$)
2. Valores candidatos ($\tilde{C}_t$) √ó Puerta de entrada ($i_t$)
3. **Suma** para formar nuevo estado ($C_t$)

### Interpretaci√≥n
Combina informaci√≥n conservada con nueva informaci√≥n importante.

## Puerta de Salida

### Funci√≥n Principal
Decide **qu√© valores pasar al siguiente paso de tiempo**.

### Ecuaci√≥n
$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$

### Proceso de Decisi√≥n
- Analiza valores y asigna importancia (-1 a 1)
- Regula datos antes del siguiente c√°lculo
- Decide **salida final** del estado actual

## Actualizaci√≥n del Estado Oculto

### Ecuaci√≥n Final
$$h_t = o_t \cdot \tanh(C_t)$$

### Funcionalidad
- Estado oculto se actualiza seg√∫n estado de celda y puerta de salida
- Se usa como **salida para paso actual**
- Sirve como **entrada para siguiente paso**

## Flujo Completo LSTM

### Proceso Secuencial
1. **Puerta de Olvido**: Qu√© informaci√≥n conservar (sigmoidea 0-1)
2. **Puerta de Entrada**: Qu√© nueva informaci√≥n incorporar (sigmoidea + tanh)
3. **Actualizaci√≥n Estado**: $C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t$
4. **Puerta de Salida**: Qu√© informaci√≥n emitir (sigmoidea)
5. **Estado Oculto**: $h_t = o_t \cdot \tanh(C_t)$

## Aplicaciones Pr√°cticas

### Campos de Uso
- **Procesamiento de lenguaje natural**
- **Reconocimiento de voz**
- **Predicci√≥n de series temporales**
- **Sistemas de recomendaci√≥n**
- **An√°lisis de datos secuenciales**

### Beneficios Clave
- **Manejan dependencias largas** eficientemente
- **Resuelven problema del gradiente de fuga**
- **Preservan contexto** en secuencias largas
- **Flexibles** para diversos datos temporales


### Impacto
Las LSTM representan **evoluci√≥n significativa** sobre RNN tradicionales, permitiendo manejo efectivo de **dependencias temporales largas**.


## ¬øQu√© es una capa LSTM en el contexto de inundaciones?

Es una capa de red neuronal recurrente especializada en procesar **datos secuenciales temporales**:

- Series de tiempo de **niveles de r√≠os**
- Datos de **precipitaci√≥n** acumulada  
- **Humedad del suelo** hist√≥rica
- Variables meteorol√≥gicas temporales

**Dise√±o con "puertas"** le permite:
- Recordar **patrones importantes** (picos de lluvia antecedentes)
- Olvidar **informaci√≥n irrelevante** (d√≠as sin lluvia)
- Predecir **eventos extremos** con horas/d√≠as de anticipaci√≥n

## Par√°metros principales (en contexto de inundaciones)

### Configuraci√≥n clave para hidrolog√≠a

**units** ‚Üí N√∫mero de celdas LSTM. M√°s unidades capturan patrones complejos  
*Ejemplo: 50 unidades para predecir crecidas en una cuenca grande*

**return_sequences** ‚Üí Si es True, devuelve salidas en cada paso temporal  
*√ötil para conectar otra LSTM despu√©s*

**return_state** ‚Üí Devuelve estados internos  
*Para modelos avanzados de atenci√≥n o inicializaci√≥n*

**activation** ‚Üí Funci√≥n de activaci√≥n  
*Por defecto tanh para estabilidad*

**input_shape** ‚Üí Forma de la entrada: (pasos_temporales, caracter√≠sticas)  
*Ejemplo: (30, 5) para 30 d√≠as con 5 variables meteorol√≥gicas*

## Ejemplo conceptual con datos reales

### Secuencia de 7 d√≠as de datos hidrol√≥gicos

**Entrada (7 d√≠as √ó 4 caracter√≠sticas):**

| D√≠a | Precipitaci√≥n | Nivel R√≠o | Humedad Suelo | Temperatura |
|-----|---------------|-----------|---------------|-------------|
| 1 | 5mm | 2.0m | 60% | 20¬∞C |
| 2 | 10mm | 2.2m | 70% | 18¬∞C |
| 3 | 25mm | 2.5m | 80% | 16¬∞C |
| 4 | 40mm | 2.8m | 85% | 15¬∞C |
| 5 | 35mm | 3.0m | 90% | 14¬∞C |
| 6 | 45mm | 3.2m | 92% | 15¬∞C |
| 7 | 50mm | 3.5m | 95% | 15¬∞C |

### Procesamiento LSTM
Una LSTM con **units=3** procesar√° esta secuencia d√≠a a d√≠a, actualizando su **"memoria"** con la evoluci√≥n de las variables.

## Ejemplo interpretativo para predicci√≥n de inundaciones

### Configuraci√≥n del modelo
`LSTM(units=10, return_sequences=False, input_shape=(30, 6))`

#### a) Entrada
30 d√≠as hist√≥ricos con 6 variables:
- Precipitaci√≥n
- Nivel del r√≠o  
- Humedad del suelo
- Temperatura
- Velocidad del viento
- Presi√≥n atmosf√©rica

#### b) Proceso

**units=10** ‚Üí La capa tiene 10 celdas LSTM para capturar patrones complejos  
*Ejemplo: correlaciones entre lluvia acumulada y subida del r√≠o*

**return_sequences=False** ‚Üí Solo devuelve la salida del √∫ltimo d√≠a  
*Para predecir inundaci√≥n al d√≠a siguiente*

**Puertas de olvido** descartan datos no relevantes  
*Ejemplo: d√≠as secos antiguos*

**Puertas de entrada** guardan informaci√≥n cr√≠tica  
*Ejemplo: lluvia intensa reciente*

#### c) Salida

**Forma**: (batch_size, 10)

**Interpretaci√≥n**: Cada valor del vector representa un patr√≥n aprendido:
- Valor 1: "alta humedad acumulada"
- Valor 2: "tendencia de subida r√°pida del r√≠o"
- Valor 3: "patr√≥n de precipitaci√≥n intensa"
- ... etc.

Este vector se alimenta a una capa densa para predecir **probabilidad de inundaci√≥n**.

## Analog√≠a intuitiva

Piensa en la LSTM como un **experto en hidrolog√≠a** que analiza un informe diario:

 Forget gate

Como cuando decide que una **lluvia leve de hace 20 d√≠as** ya no es relevante para el riesgo actual.

Input gate  
Cuando subraya datos importantes: **"¬°Lluvia de 100mm en 3 d√≠as!"**

 Output gate
Cuando emite un alerta gradual: **"R√≠o subiendo 0.5m/d√≠a ‚Üí riesgo en 48h"**

Evoluci√≥n del an√°lisis

- **Capas simples**: Variables individuales

- **Capas m√∫ltiples**: Interacciones complejas

- **Ejemplo avanzado**: "Suelo saturado + lluvia intensa = inundaci√≥n repentina"

## Mini-ejemplo num√©rico simplificado

### Entrada de 3 d√≠as para predecir crecida

| D√≠a | Precipitaci√≥n | Nivel R√≠o | Normalizado |
|-----|---------------|-----------|-------------|
| 1 | 10mm | 1.5m | [0.1, 0.15] |
| 2 | 30mm | 1.7m | [0.3, 0.17] |
| 3 | 60mm | 2.0m | [0.6, 0.2] |

### Procesamiento paso a paso

**Estado inicial**: h‚ÇÄ = [0, 0], c‚ÇÄ = [0, 0]

**Paso 1 (D√≠a 1)**:
- LSTM detecta lluvia moderada ‚Üí actualiza ligeramente su estado
- h‚ÇÅ = [0.02, 0.01], c‚ÇÅ = [0.03, 0.02]

**Paso 2 (D√≠a 2)**:
- Lluvia alta ‚Üí puerta de entrada guarda esta informaci√≥n
- h‚ÇÇ = [0.15, 0.1], c‚ÇÇ = [0.2, 0.12]

**Paso 3 (D√≠a 3)**:
- Lluvia extrema + nivel subiendo ‚Üí puerta de olvido mantiene todo el historial reciente
- **Salida**: h‚ÇÉ = [0.8, 0.6] (indica alto riesgo)

### Resultado final
Una capa densa traduce esto a: **Probabilidad_inundaci√≥n = 85%**

## ¬øPor qu√© es √∫til para inundaciones?

### Captura la no linealidad de las cuencas
*Ejemplo: el suelo se satura despu√©s de X d√≠as de lluvia*

### Modela retrasos temporales  
*Ejemplo: la lluvia tarda horas en afectar el r√≠o aguas abajo*

### Puede integrar m√∫ltiples fuentes de datos
- Sat√©lites
- Sensores terrestres  
- Pron√≥sticos meteorol√≥gicos

### Usada en sistemas de alerta temprana
Como el **EFAS** (European Flood Awareness System)

## Resumen de ventajas

### Para predicci√≥n hidrol√≥gica
- **Memoria persistente** para eventos lejanos cr√≠ticos
- **Adaptabilidad** a diferentes tipos de cuencas
- **Robustez** con datos imperfectos o incompletos

### Para gesti√≥n de emergencias
- **Alertas tempranas** con mayor anticipaci√≥n
- **Mejor precisi√≥n** en predicci√≥n de eventos extremos
- **Integraci√≥n** de m√∫ltiples fuentes de informaci√≥n

## Aplicaciones en sistemas reales

### Casos de implementaci√≥n
- **Sistemas de alerta temprana** nacionales y regionales
- **Monitoreo de cuencas** cr√≠ticas
- **Predicci√≥n de crecidas** repentinas
- **Gesti√≥n de embalses** y recursos h√≠dricos

### Beneficios demostrados
- **Reducci√≥n de da√±os** econ√≥micos
- **Protecci√≥n de vidas** humanas
- **Optimizaci√≥n** de recursos de emergencia
- **Mejora** en la toma de decisiones

## Generaci√≥n de Datos de Inundaciones

```{python}
#| label: gen-datos
#| echo: true
#| warning: false
#| message: false
#| fig-cap: "Generaci√≥n de datos sint√©ticos para predicci√≥n de inundaciones"
#| code-fold: false

# =============================
# 1. IMPORTAR LIBRER√çAS Y GENERAR DATOS DE INUNDACIONES
# ===============================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from tensorflow.keras.metrics import Precision, Recall

# Configurar semilla para reproducibilidad
np.random.seed(123)
tf.random.set_seed(123)

# Generar datos 
n = 1500  # M√°s datos para mejor entrenamiento LSTM

lluvia_total = np.random.gamma(shape=3, scale=25, size=n)
intensidad_lluvia = np.random.gamma(shape=2, scale=15, size=n)
duracion_lluvia = np.random.lognormal(mean=np.log(2), sigma=0.4, size=n)
capacidad_drenaje = np.random.lognormal(mean=np.log(60), sigma=0.3, size=n)
impermeabilidad = np.random.beta(a=5, b=2, size=n)

# Calcular riesgo 
riesgo = (
    0.6 * (lluvia_total / 80) +           # Escala mejorada
    0.5 * (intensidad_lluvia / 60) +      # M√°s peso a intensidad
    0.6 * (impermeabilidad * 1.2) +       # Impermeabilidad escalada
    -0.7 * (capacidad_drenaje / 50) +     # M√°s impacto del drenaje
    0.4 * (duracion_lluvia / 5)           # Duraci√≥n con mejor escala
)

# Probabilidad de inundaci√≥n
prob_inundacion = 1 / (1 + np.exp(-5 * (riesgo - 0.5)))
zona_inundada = np.random.binomial(1, prob_inundacion)

# Crear DataFrame
df = pd.DataFrame({
    'lluvia_total': lluvia_total,
    'intensidad_lluvia': intensidad_lluvia,
    'duracion_lluvia': duracion_lluvia,
    'capacidad_drenaje': capacidad_drenaje,
    'impermeabilidad': impermeabilidad,
    'zona_inundada': zona_inundada
})

print("‚úÖ Datos generados exitosamente")
print(f"üìä Dimensiones: {df.shape}")
print(f"üéØ Balance de clases: {df['zona_inundada'].value_counts().to_dict()}")
```

```{python}
#| label: stats-datos
#| echo: true
#| warning: false
#| message: false
#| fig-cap: "Estad√≠sticas descriptivas de los datos generados"

print("\nüìà ESTAD√çSTICAS DESCRIPTIVAS:")
print(df.describe().round(3))
```

```{python}
#| label: head-datos
#| echo: true
#| warning: false
#| message: false

print("\nüìã MUESTRA DE DATOS (primeras 5 filas):")
print(df.head().round(3))
```

```{python}
# =============================================================================
# 2. PREPARACI√ìN DE DATOS - SIN SECUENCIAS TEMPORALES
# =============================================================================

features = ['lluvia_total', 'intensidad_lluvia', 'duracion_lluvia', 'capacidad_drenaje', 'impermeabilidad']
target = 'zona_inundada'

# Normalizar caracter√≠sticas
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(df[features])
y = df[target].values

# Divisi√≥n tradicional (NO temporal)
split_idx = int(0.8 * len(X_scaled))
X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

print(f"üìä DIVISI√ìN DE DATOS :")
print(f"Entrenamiento: {X_train.shape[0]} muestras")
print(f"Prueba: {X_test.shape[0]} muestras")
print(f"Caracter√≠sticas: {X_train.shape[1]}")
```

```{python}
# =============================================================================
# 3. MODELO DENSE SIMPLE  - CON CLASS WEIGHT
# =============================================================================
class SimpleDenseClassifier:
    def __init__(self, n_features, x_train, y_train, x_test, y_test):
        self.model = None
        self.n_features = n_features
        self.x_train = x_train
        self.y_train = y_train
        self.x_test = x_test
        self.y_test = y_test
        self.history = None
    
    def build_model(self, hidden_units=32, dropout_rate=0.3, learning_rate=0.001):
        """
        Construye modelo Dense simple para clasificaci√≥n
        """
        self.model = Sequential([
            Input(shape=(self.n_features,)),
            Dense(hidden_units, activation='relu'),
            Dropout(dropout_rate),
            Dense(hidden_units//2, activation='relu'),
            Dropout(0.1),
            Dense(1, activation='sigmoid')
        ])
        
        optimizer = Adam(learning_rate=learning_rate)
        self.model.compile(
            optimizer=optimizer,
            loss='binary_crossentropy',
             metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]
        )
        
        return self.model
    
    def train(self, epochs=100, batch_size=32, verbose=1, class_weight=None):
        """
        Entrena el modelo CON CLASS WEIGHT
        """
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        )
        
        self.history = self.model.fit(
            self.x_train, self.y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(self.x_test, self.y_test),
            callbacks=[early_stopping],
            verbose=verbose,
            shuffle=True,
            class_weight=class_weight  # LINEA NUEVA
        )
        
        return self.history
    
    def evaluate(self):
        """
        Eval√∫a el modelo
        """
        print("\nüìä EVALUACI√ìN DEL MODELO DENSE")
        print("=" * 50)
        
        loss, accuracy, precision, recall = self.model.evaluate(self.x_test, self.y_test, verbose=0)
        y_pred_proba = self.model.predict(self.x_test, verbose=0)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        print(f"Precisi√≥n: {accuracy:.4f}")
        print(f"P√©rdida: {loss:.4f}")
        print(f"Precisi√≥n (metric): {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        
        print("\nüìà REPORTE DE CLASIFICACI√ìN:")
        print(classification_report(self.y_test, y_pred, 
                                  target_names=['No Inundaci√≥n', 'Inundaci√≥n']))
        
        return y_pred, y_pred_proba
    
    def plot_training_history(self):
        """
        Grafica la evoluci√≥n del entrenamiento
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # P√©rdida
        ax1.plot(self.history.history['loss'], label='Entrenamiento')
        ax1.plot(self.history.history['val_loss'], label='Validaci√≥n')
        ax1.set_title('Evoluci√≥n de la P√©rdida')
        ax1.set_xlabel('√âpoca')
        ax1.set_ylabel('P√©rdida')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Precisi√≥n
        ax2.plot(self.history.history['accuracy'], label='Entrenamiento')
        ax2.plot(self.history.history['val_accuracy'], label='Validaci√≥n')
        ax2.set_title('Evoluci√≥n de la Precisi√≥n')
        ax2.set_xlabel('√âpoca')
        ax2.set_ylabel('Precisi√≥n')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# Crear y entrenar modelo simple
dense_classifier = SimpleDenseClassifier(
    n_features=len(features),
    x_train=X_train,
    y_train=y_train,
    x_test=X_test,
    y_test=y_test
)

# Construir modelo
model = dense_classifier.build_model(
    hidden_units=32,
    dropout_rate=0.3,
    learning_rate=0.001
)

print("üèóÔ∏è ARQUITECTURA DEL MODELO DENSE:")
model.summary()
```
```{python}
# =============================================================================
#   ARQUITECTURA CON DROPOUT INTEGRADO
# =============================================================================
# echo : false 
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np

def visualizar_arquitectura_corregida():
    fig, ax = plt.subplots(1, 1, figsize=(16, 10))
    
    # Configuraci√≥n
    ax.set_xlim(0, 12)
    ax.set_ylim(0, 15)
    ax.set_aspect('equal')
    ax.axis('off')
    
    # T√≠tulo
    ax.text(6, 14.5, 'ARQUITECTURA DE LA RED NEURONAL - MODELO DENSE', 
            ha='center', va='center', fontsize=18, fontweight='bold')
    
    # Colores
    colores = {
        'entrada': 'lightblue',
        'oculta': 'lightgreen', 
        'dropout': 'orange',
        'salida': 'red',
        'prediccion': 'yellow'
    }
    
    # ==================== CAPA DE ENTRADA ====================
    ax.text(1, 12, 'CAPA DE ENTRADA\n(5 caracter√≠sticas)', 
            ha='center', va='center', fontsize=12, fontweight='bold',
            bbox=dict(boxstyle="round,pad=0.3", facecolor=colores['entrada']))
    
    # Neuronas de entrada (5 caracter√≠sticas)
    for i in range(5):
        y_pos = 11 - i * 0.8
        ax.plot(1, y_pos, 'o', markersize=10, color='blue')
        # Conexiones a capa oculta 1
        for j in range(32):
            target_y = 10 - (j * 0.25)  # 32 neuronas en capa 1
            ax.plot([1.1, 2.9], [y_pos, target_y], 'gray', alpha=0.1, linewidth=0.5)
    
    # ==================== CAPA OCULTA 1 ====================
    ax.text(4, 12, 'CAPA OCULTA 1\n(32 neuronas + Dropout 30%)', 
            ha='center', va='center', fontsize=12, fontweight='bold',
            bbox=dict(boxstyle="round,pad=0.3", facecolor=colores['oculta']))
    
    # 32 neuronas capa 1
    neuronas_capa1 = []
    for i in range(32):
        y_pos = 11 - i * 0.25
        neuronas_capa1.append(y_pos)
        # Neurona activa (70%)
        if i % 3 != 0:  # 70% de neuronas activas
            ax.plot(4, y_pos, 'o', markersize=6, color='green')
        else:  # 30% dropout (apagadas)
            ax.plot(4, y_pos, 'x', markersize=8, color='red')
    
    # Conexiones a capa oculta 2
    for i, y1 in enumerate(neuronas_capa1):
        if i % 3 != 0:  # Solo neuronas activas se conectan
            for j in range(16):
                target_y = 9 - (j * 0.4)  # 16 neuronas en capa 2
                ax.plot([4.1, 5.9], [y1, target_y], 'gray', alpha=0.1, linewidth=0.5)
    
    # ==================== CAPA OCULTA 2 ====================
    ax.text(7, 10, 'CAPA OCULTA 2\n(16 neuronas + Dropout 30%)', 
            ha='center', va='center', fontsize=12, fontweight='bold',
            bbox=dict(boxstyle="round,pad=0.3", facecolor=colores['oculta']))
    
    # 16 neuronas capa 2
    neuronas_capa2 = []
    for i in range(16):
        y_pos = 9 - i * 0.4
        neuronas_capa2.append(y_pos)
        # Neurona activa (70%)
        if i % 3 != 0:  # 70% de neuronas activas
            ax.plot(7, y_pos, 'o', markersize=7, color='green')
        else:  # 30% dropout (apagadas)
            ax.plot(7, y_pos, 'x', markersize=9, color='red')
    
    # Conexiones a capa salida
    for i, y2 in enumerate(neuronas_capa2):
        if i % 3 != 0:  # Solo neuronas activas se conectan
            ax.plot([7.1, 8.9], [y2, 6], 'gray', alpha=0.3, linewidth=1)
    
    # ==================== CAPA DE SALIDA ====================
    ax.text(10, 7, 'CAPA DE SALIDA\n(1 neurona)', 
            ha='center', va='center', fontsize=12, fontweight='bold',
            bbox=dict(boxstyle="round,pad=0.3", facecolor=colores['salida']))
    
    # Neurona de salida
    ax.plot(10, 6, 'o', markersize=12, color='red')
    
    # ==================== PREDICCI√ìN ====================
    ax.text(10, 4, 'PREDICCI√ìN FINAL\n0% - 100% probabilidad\nINUNDACI√ìN / NO INUNDACI√ìN', 
            ha='center', va='center', fontsize=11, fontweight='bold',
            bbox=dict(boxstyle="round,pad=0.3", facecolor=colores['prediccion']))
    
    # ==================== FLUJOS PRINCIPALES ====================
    # Flechas entre capas
    ax.arrow(1.5, 10, 1.8, 0, head_width=0.2, head_length=0.2, fc='black', ec='black')
    ax.text(2.5, 10.3, '5 ‚Üí 32 conexiones', ha='center', va='center', fontsize=9)
    
    ax.arrow(4.5, 8, 1.8, 0, head_width=0.2, head_length=0.2, fc='black', ec='black')
    ax.text(5.5, 8.3, '32 ‚Üí 16 conexiones', ha='center', va='center', fontsize=9)
    
    ax.arrow(7.5, 6, 1.8, 0, head_width=0.2, head_length=0.2, fc='black', ec='black')
    ax.text(8.5, 6.3, '16 ‚Üí 1 conexi√≥n', ha='center', va='center', fontsize=9)
    
    # ==================== DETALLES T√âCNICOS ====================
    detalles = [
        "‚Ä¢ Cada neurona se conecta con TODAS las de la siguiente capa",
        "‚Ä¢ Dropout: 30% de neuronas se APAGAN aleatoriamente en entrenamiento",
        "‚Ä¢ Activaci√≥n: ReLU en capas ocultas, Sigmoid en salida",
        "‚Ä¢ Entrada: 5 caracter√≠sticas ‚Üí Salida: 1 probabilidad"
    ]
    
    for i, detalle in enumerate(detalles):
        ax.text(6, 2 - i*0.4, detalle, ha='center', va='center', fontsize=10)
    
    plt.tight_layout()
    plt.show()

# Ejecutar visualizaci√≥n 
print("ARQUITECTURA CON DROPOUT INTEGRADO")
visualizar_arquitectura_corregida()
```
```{python}
# =============================================================================
# 4. ENTRENAMIENTO Y EVALUACI√ìN  - CON PESOS BALANCEADOS
# =============================================================================
#| results: 'hide'
#| message: false
#| warning: false
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Calcular pesos autom√°ticamente para balancear clases
class_weights = compute_class_weight(
    'balanced', 
    classes=np.unique(y_train), 
    y=y_train
)
class_weight_dict = {0: class_weights[0], 1: class_weights[1]}

print(f"‚öñÔ∏è PESOS BALANCEADOS CALCULADOS:")
print(f"Peso Clase 0 (No inundaci√≥n): {class_weight_dict[0]:.3f}")
print(f"Peso Clase 1 (Inundaci√≥n): {class_weight_dict[1]:.3f}")

# Entrenar modelo CON PESOS BALANCEADOS
history = dense_classifier.train(
    epochs=200, 
    batch_size=32, 
    verbose=1,
    class_weight=class_weight_dict  # LINEA MAGICA
)

# Graficar evoluci√≥n del entrenamiento
dense_classifier.plot_training_history()

# Evaluar modelo
y_pred, y_pred_proba = dense_classifier.evaluate()

# Matriz de confusi√≥n
print("\nüéØ MATRIZ DE CONFUSI√ìN:")
cm = confusion_matrix(y_test, y_pred)
fig, ax = plt.subplots(figsize=(8, 6))
im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
ax.figure.colorbar(im, ax=ax)
ax.set(xticks=[0, 1], yticks=[0, 1],
       xticklabels=['No Inundaci√≥n', 'Inundaci√≥n'], 
       yticklabels=['No Inundaci√≥n', 'Inundaci√≥n'],
       title='Matriz de Confusi√≥n - Modelo Dense',
       ylabel='Real', xlabel='Predicho')

# Anotar valores en la matriz
thresh = cm.max() / 2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        ax.text(j, i, format(cm[i, j], 'd'),
                ha="center", va="center",
                color="white" if cm[i, j] > thresh else "black")
plt.tight_layout()
plt.show()
```

```{python}
# =============================================================================
# 5. VISUALIZACI√ìN DE PREDICCIONES INDIVIDUALES
# =============================================================================
# echo: false
print("\nüîç PRIMERAS 6 PREDICCIONES vs REALIDAD:")
print("=" * 50)

# Crear un DataFrame para visualizaci√≥n
resultados_df = pd.DataFrame({
    'Muestra': range(1, 7),
    'Lluvia_Total': X_test[:6, 0] * (df['lluvia_total'].max() - df['lluvia_total'].min()) + df['lluvia_total'].min(),
    'Intensidad': X_test[:6, 1] * (df['intensidad_lluvia'].max() - df['intensidad_lluvia'].min()) + df['intensidad_lluvia'].min(),
    'Drenaje': X_test[:6, 3] * (df['capacidad_drenaje'].max() - df['capacidad_drenaje'].min()) + df['capacidad_drenaje'].min(),
    'Real': y_test[:6],
    'Predicho': y_pred[:6],
    'Probabilidad': y_pred_proba[:6].flatten()
})

# Mostrar tabla
for i in range(6):
    real_str = "üîµ INUNDACI√ìN" if resultados_df['Real'].iloc[i] == 1 else "‚ö´ NO INUNDACI√ìN"
    pred_str = "üî¥ INUNDACI√ìN" if resultados_df['Predicho'].iloc[i] == 1 else "‚ö™ NO INUNDACI√ìN"
    prob = resultados_df['Probabilidad'].iloc[i]
    
    print(f"Muestra {i+1}:")
    print(f"  Lluvia: {resultados_df['Lluvia_Total'].iloc[i]:.1f}mm | Intensidad: {resultados_df['Intensidad'].iloc[i]:.1f}mm/h")
    print(f"  Drenaje: {resultados_df['Drenaje'].iloc[i]:.1f}% | Prob: {prob:.3f}")
    print(f"  Real: {real_str}")
    print(f"  Pred: {pred_str}")
    print(f"  {'‚úÖ ACIERTO' if resultados_df['Real'].iloc[i] == resultados_df['Predicho'].iloc[i] else '‚ùå ERROR'}")
    print("-" * 40)

# Gr√°fica de comparaci√≥n
plt.figure(figsize=(12, 6))

# Gr√°fica 1: Valores reales vs predichos
plt.subplot(1, 2, 1)
indices = range(1, 7)
plt.plot(indices, y_test[:6], 'bo-', label='Real', linewidth=2, markersize=8)
plt.plot(indices, y_pred[:6], 'ro--', label='Predicho', linewidth=2, markersize=8)
plt.plot(indices, y_pred_proba[:6], 'gx-', label='Probabilidad', linewidth=1, markersize=6)

plt.title('Comparaci√≥n: Real vs Predicho\n(Primeras 6 muestras)')
plt.xlabel('N√∫mero de Muestra')
plt.ylabel('Valor')
plt.xticks(indices)
plt.legend()
plt.grid(True, alpha=0.3)

# Gr√°fica 2: Probabilidades con umbral
plt.subplot(1, 2, 2)
bars = plt.bar(indices, y_pred_proba[:6].flatten(), color=['red' if x > 0.5 else 'blue' for x in y_pred_proba[:6]])
plt.axhline(y=0.5, color='black', linestyle='--', label='Umbral (0.5)')
plt.title('Probabilidades de Predicci√≥n\n(Rojo > 0.5 = Inundaci√≥n)')
plt.xlabel('N√∫mero de Muestra')
plt.ylabel('Probabilidad')
plt.xticks(indices)
plt.legend()

# A√±adir valores en las barras
for bar, prob, real, pred in zip(bars, y_pred_proba[:6], y_test[:6], y_pred[:6]):
    height = bar.get_height()
    color = 'green' if real == pred else 'black'
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,
             f'{prob[0]:.3f}\n({"‚úÖ" if real == pred else "‚ùå"})',
             ha='center', va='bottom', color=color, fontweight='bold')

plt.tight_layout()
plt.show()
```
## Accuracy

### Bootstrap
Regresi√≥n log√≠stica = 0.8
√Årbol de desici√≥n = 0.85
Bagging 30 √°rboles = 0.83

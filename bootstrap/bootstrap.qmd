---
title: "Redes LSTM: Memoria a Largo y Corto Plazo"
subtitle: "Arquitectura y Funcionamiento Detallado"
author: "Presentaci√≥n sobre Redes Neuronales Recurrentes"
date: "Noviembre 2023"
format:
  beamer:
    keep-tex: true
    slide-level: 2
    theme: "Madrid"
    colortheme: "dolphin"
  html:
    theme: "cosmo"
    toc: true
    toc-depth: 2
    css: styles.css
---

## ¬øQu√© son las LSTM?

- **Extensi√≥n de las RNN** dise√±adas para capturar dependencias temporales de secuencias largas
- Las RNN b√°sicas **luchan por capturar** dependencias largas en el tiempo
- Introducen el concepto de **"celda de memoria"** que puede mantener, escribir o leer informaci√≥n
- Controladas por **estructuras llamadas puertas** que gestionan el flujo de informaci√≥n

## Problemas que Resuelven las LSTM

### Limitaciones de las RNN B√°sicas
- **Problema del gradiente de fuga**
- Dificultad para considerar **secuencias de entrada largas**
- **P√©rdida de contexto** en dependencias temporales largas

### Soluciones de las LSTM
- Las alteraciones en los LSTM **abordan el problema del gradiente de fuga**
- Permiten considerar **secuencias de entrada mucho m√°s largas**
- **Preservan el contexto** de manera m√°s efectiva

## Arquitectura LSTM: Las Tres Puertas

### Componentes Fundamentales
1. **Puerta de entrada** - Decide qu√© valores son importantes
2. **Puerta de olvido** - Descarta informaci√≥n innecesaria  
3. **Puerta de salida** - Decide qu√© informaci√≥n pasar al siguiente paso

### Caracter√≠sticas Comunes
- Consideran las **entradas del paso de tiempo anterior**
- Modifican la **memoria del modelo** y los **pesos de entrada**
- Utilizan **funciones de activaci√≥n espec√≠ficas** para cada puerta

## Puerta de Olvido

### Funci√≥n Principal
Descarta **informaci√≥n que el modelo considera innecesaria** para tomar decisiones.

### Ecuaci√≥n Matem√°tica
$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

### Componentes
- $W_f$: **peso de la puerta del olvido**
- $b_f$: **sesgo de la puerta de olvido**
- $\sigma$: **funci√≥n sigmoidea** (valores 0-1)

### Interpretaci√≥n
- **0**: Olvidar completamente
- **1**: Retener completamente
- Determina qu√© informaci√≥n del estado anterior conservar

## Puerta de Entrada

### Funci√≥n Principal
Decide **qu√© valores son importantes** y deben transmitirse por el modelo.

### Ecuaciones
**Decidir qu√© actualizar:**
$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$

**Generar valores candidatos:**
$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

### Funciones de Activaci√≥n
- **Sigmoidea**: decide qu√© valores transmitir (0 = reducir, 1 = conservar)
- **TanH**: decide importancia de valores (-1 a 1)

## C√©lula de Memoria Candidata

### Funci√≥n
Genera **nueva informaci√≥n potencial** para almacenar en el estado de la celda.

### Ecuaci√≥n
$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

### Caracter√≠sticas
- Genera **valores candidatos** para agregar al estado
- Funci√≥n **tanh** asegura valores entre -1 y 1
- Representa **informaci√≥n potencial** para almacenar

## Actualizaci√≥n del Estado de la Celda

### Proceso de Actualizaci√≥n
$$C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t$$

### Explicaci√≥n del Proceso
1. Estado anterior ($C_{t-1}$) √ó Puerta de olvido ($f_t$)
2. Valores candidatos ($\tilde{C}_t$) √ó Puerta de entrada ($i_t$)
3. **Suma** para formar nuevo estado ($C_t$)

### Interpretaci√≥n
Combina informaci√≥n conservada con nueva informaci√≥n importante.

## Puerta de Salida

### Funci√≥n Principal
Decide **qu√© valores pasar al siguiente paso de tiempo**.

### Ecuaci√≥n
$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$

### Proceso de Decisi√≥n
- Analiza valores y asigna importancia (-1 a 1)
- Regula datos antes del siguiente c√°lculo
- Decide **salida final** del estado actual

## Actualizaci√≥n del Estado Oculto

### Ecuaci√≥n Final
$$h_t = o_t \cdot \tanh(C_t)$$

### Funcionalidad
- Estado oculto se actualiza seg√∫n estado de celda y puerta de salida
- Se usa como **salida para paso actual**
- Sirve como **entrada para siguiente paso**

## Flujo Completo LSTM

### Proceso Secuencial
1. **Puerta de Olvido**: Qu√© informaci√≥n conservar (sigmoidea 0-1)
2. **Puerta de Entrada**: Qu√© nueva informaci√≥n incorporar (sigmoidea + tanh)
3. **Actualizaci√≥n Estado**: $C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t$
4. **Puerta de Salida**: Qu√© informaci√≥n emitir (sigmoidea)
5. **Estado Oculto**: $h_t = o_t \cdot \tanh(C_t)$

## Aplicaciones Pr√°cticas

### Campos de Uso
- **Procesamiento de lenguaje natural**
- **Reconocimiento de voz**
- **Predicci√≥n de series temporales**
- **Sistemas de recomendaci√≥n**
- **An√°lisis de datos secuenciales**

### Beneficios Clave
- **Manejan dependencias largas** eficientemente
- **Resuelven problema del gradiente de fuga**
- **Preservan contexto** en secuencias largas
- **Flexibles** para diversos datos temporales

## Resumen y Conclusiones

### Puntos Clave
- **Tres puertas especializadas** gestionan flujo de informaci√≥n
- **Sistema de memoria** que preserva contexto relevante
- **Combinaci√≥n de funciones** de activaci√≥n (sigmoidea y tanh)
- **Actualizaci√≥n controlada** del estado interno

### Impacto
Las LSTM representan **evoluci√≥n significativa** sobre RNN tradicionales, permitiendo manejo efectivo de **dependencias temporales largas**.

üîπ 2. ¬øQu√© es una capa LSTM en el contexto de inundaciones?
Es una capa de red neuronal recurrente especializada en procesar datos secuenciales temporales (como series de tiempo de niveles de r√≠os, precipitaci√≥n, humedad del suelo, etc.). Su dise√±o con "puertas" le permite recordar patrones importantes a lo largo del tiempo (ej.: picos de lluvia que anteceden a una inundaci√≥n), olvidando informaci√≥n irrelevante (ej.: d√≠as sin lluvia). Esto es crucial para predecir eventos extremos con horas o d√≠as de anticipaci√≥n.

üîπ 2.2. Par√°metros principales (en contexto de inundaciones)

units ‚Üí N√∫mero de celdas LSTM. M√°s unidades capturan patrones complejos (ej.: 50 unidades para predecir crecidas en una cuenca grande).

return_sequences ‚Üí Si es True, devuelve salidas en cada paso temporal (√∫til para conectar otra LSTM despu√©s).

return_state ‚Üí Devuelve estados internos (para modelos avanzados de atenci√≥n o inicializaci√≥n).

activation ‚Üí Funci√≥n de activaci√≥n (por defecto tanh para estabilidad).

input_shape ‚Üí Forma de la entrada: (pasos_temporales, caracter√≠sticas) (ej.: (30, 5) para 30 d√≠as con 5 variables meteorol√≥gicas).

üîπ 2.3. Ejemplo conceptual con datos reales
Imagina una secuencia de 7 d√≠as de datos hidrol√≥gicos para predecir inundaciones:

Entrada (7 d√≠as √ó 4 caracter√≠sticas):

D√≠a 1: [Precipitaci√≥n=5mm, Nivel_r√≠o=2m, Humedad_suelo=60%, Temperatura=20¬∞C]

D√≠a 2: [10mm, 2.2m, 70%, 18¬∞C]

...

D√≠a 7: [50mm, 3.5m, 95%, 15¬∞C]

Una LSTM con units=3 procesar√° esta secuencia d√≠a a d√≠a, actualizando su "memoria" con la evoluci√≥n de las variables.

üîπ 2.4. Ejemplo interpretativo para predicci√≥n de inundaciones
Sup√≥n que usamos:

python
LSTM(units=10, return_sequences=False, input_shape=(30, 6))
a) Entrada
30 d√≠as hist√≥ricos con 6 variables: precipitaci√≥n, nivel del r√≠o, humedad del suelo, temperatura, velocidad del viento, presi√≥n atmosf√©rica.

b) Proceso

units=10 ‚Üí La capa tiene 10 celdas LSTM para capturar patrones complejos (ej.: correlaciones entre lluvia acumulada y subida del r√≠o).

return_sequences=False ‚Üí Solo devuelve la salida del √∫ltimo d√≠a (para predecir inundaci√≥n al d√≠a siguiente).

Las puertas de olvido descartan datos no relevantes (ej.: d√≠as secos antiguos).

Las puertas de entrada guardan informaci√≥n cr√≠tica (ej.: lluvia intensa reciente).

La salida genera un vector de 10 valores que resume el riesgo.

c) Salida

Forma: (batch_size, 10)

Interpretaci√≥n: Cada valor del vector representa un patr√≥n aprendido (ej.: el valor 1 podr√≠a correlacionarse con "alta humedad acumulada", el valor 2 con "tendencia de subida r√°pida del r√≠o", etc.). Este vector se alimenta a una capa densa para predecir probabilidad de inundaci√≥n.

üîπ 2.5. Analog√≠a intuitiva
Piensa en la LSTM como un experto en hidrolog√≠a que analiza un informe diario:

Forget gate: Como cuando decide que una lluvia leve de hace 20 d√≠as ya no es relevante para el riesgo actual.

Input gate: Cuando subraya datos importantes: "¬°Lluvia de 100mm en 3 d√≠as!".

Output gate: Cuando emite un alerta gradual: "R√≠o subiendo 0.5m/d√≠a ‚Üí riesgo en 48h".
Cuantas m√°s capas LSTM, m√°s sofisticado es el an√°lisis (de variables individuales a interacciones complejas como "suelo saturado + lluvia intensa = inundaci√≥n repentina").

üîπ Mini-ejemplo num√©rico simplificado
Entrada de 3 d√≠as para predecir crecida:

D√≠a 1: [Precip=10mm, Nivel_r√≠o=1.5m] ‚Üí x1 = [0.1, 0.15] (valores normalizados)

D√≠a 2: [Precip=30mm, Nivel_r√≠o=1.7m] ‚Üí x2 = [0.3, 0.17]

D√≠a 3: [Precip=60mm, Nivel_r√≠o=2.0m] ‚Üí x3 = [0.6, 0.2]

Estado inicial: h0 = [0, 0], c0 = [0, 0]

Paso 1 (D√≠a 1):

La LSTM detecta lluvia moderada ‚Üí actualiza ligeramente su estado.

h1 = [0.02, 0.01], c1 = [0.03, 0.02]

Paso 2 (D√≠a 2):

Lluvia alta ‚Üí puerta de entrada guarda esta informaci√≥n.

h2 = [0.15, 0.1], c2 = [0.2, 0.12]

Paso 3 (D√≠a 3):

Lluvia extrema + nivel subiendo ‚Üí puerta de olvido mantiene todo el historial reciente.

Salida: h3 = [0.8, 0.6] (indica alto riesgo)

Una capa densa traduce esto a: Probabilidad_inundaci√≥n = 85%

üîπ ¬øPor qu√© es √∫til para inundaciones?

Captura la no linealidad de las cuencas (ej.: el suelo se satura despu√©s de X d√≠as de lluvia).

Modela retrasos temporales (ej.: la lluvia tarda horas en afectar el r√≠o aguas abajo).

Puede integrar m√∫ltiples fuentes de datos (sat√©lites, sensores, pron√≥sticos meteorol√≥gicos).

Usada en sistemas de alerta temprana como el EFAS (European Flood Awareness System).








```{python}
# =============================================================================
# - ARQUITECTURA CON DROPOUT INTEGRADO
# =============================================================================
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np

def visualizar_arquitectura_corregida():
    fig, ax = plt.subplots(1, 1, figsize=(16, 10))
    
    # Configuraci√≥n
    ax.set_xlim(0, 12)
    ax.set_ylim(0, 15)
    ax.set_aspect('equal')
    ax.axis('off')
    
    # T√≠tulo
    ax.text(6, 14.5, 'ARQUITECTURA DE LA RED NEURONAL - MODELO DENSE', 
            ha='center', va='center', fontsize=18, fontweight='bold')
    
    # Colores
    colores = {
        'entrada': 'lightblue',
        'oculta': 'lightgreen', 
        'dropout': 'orange',
        'salida': 'red',
        'prediccion': 'yellow'
    }
    
    # ==================== CAPA DE ENTRADA ====================
    ax.text(1, 12, 'CAPA DE ENTRADA\n(5 caracter√≠sticas)', 
            ha='center', va='center', fontsize=12, fontweight='bold',
            bbox=dict(boxstyle="round,pad=0.3", facecolor=colores['entrada']))
    
    # Neuronas de entrada (5 caracter√≠sticas)
    for i in range(5):
        y_pos = 11 - i * 0.8
        ax.plot(1, y_pos, 'o', markersize=10, color='blue')
        # Conexiones a capa oculta 1
        for j in range(32):
            target_y = 10 - (j * 0.25)  # 32 neuronas en capa 1
            ax.plot([1.1, 2.9], [y_pos, target_y], 'gray', alpha=0.1, linewidth=0.5)
    
    # ==================== CAPA OCULTA 1 ====================
    ax.text(4, 12, 'CAPA OCULTA 1\n(32 neuronas + Dropout 30%)', 
            ha='center', va='center', fontsize=12, fontweight='bold',
            bbox=dict(boxstyle="round,pad=0.3", facecolor=colores['oculta']))
    
    # 32 neuronas capa 1
    neuronas_capa1 = []
    for i in range(32):
        y_pos = 11 - i * 0.25
        neuronas_capa1.append(y_pos)
        # Neurona activa (70%)
        if i % 3 != 0:  # 70% de neuronas activas
            ax.plot(4, y_pos, 'o', markersize=6, color='green')
        else:  # 30% dropout (apagadas)
            ax.plot(4, y_pos, 'x', markersize=8, color='red')
    
    # Conexiones a capa oculta 2
    for i, y1 in enumerate(neuronas_capa1):
        if i % 3 != 0:  # Solo neuronas activas se conectan
            for j in range(16):
                target_y = 9 - (j * 0.4)  # 16 neuronas en capa 2
                ax.plot([4.1, 5.9], [y1, target_y], 'gray', alpha=0.1, linewidth=0.5)
    
    # ==================== CAPA OCULTA 2 ====================
    ax.text(7, 10, 'CAPA OCULTA 2\n(16 neuronas + Dropout 30%)', 
            ha='center', va='center', fontsize=12, fontweight='bold',
            bbox=dict(boxstyle="round,pad=0.3", facecolor=colores['oculta']))
    
    # 16 neuronas capa 2
    neuronas_capa2 = []
    for i in range(16):
        y_pos = 9 - i * 0.4
        neuronas_capa2.append(y_pos)
        # Neurona activa (70%)
        if i % 3 != 0:  # 70% de neuronas activas
            ax.plot(7, y_pos, 'o', markersize=7, color='green')
        else:  # 30% dropout (apagadas)
            ax.plot(7, y_pos, 'x', markersize=9, color='red')
    
    # Conexiones a capa salida
    for i, y2 in enumerate(neuronas_capa2):
        if i % 3 != 0:  # Solo neuronas activas se conectan
            ax.plot([7.1, 8.9], [y2, 6], 'gray', alpha=0.3, linewidth=1)
    
    # ==================== CAPA DE SALIDA ====================
    ax.text(10, 7, 'CAPA DE SALIDA\n(1 neurona)', 
            ha='center', va='center', fontsize=12, fontweight='bold',
            bbox=dict(boxstyle="round,pad=0.3", facecolor=colores['salida']))
    
    # Neurona de salida
    ax.plot(10, 6, 'o', markersize=12, color='red')
    
    # ==================== PREDICCI√ìN ====================
    ax.text(10, 4, 'PREDICCI√ìN FINAL\n0% - 100% probabilidad\nINUNDACI√ìN / NO INUNDACI√ìN', 
            ha='center', va='center', fontsize=11, fontweight='bold',
            bbox=dict(boxstyle="round,pad=0.3", facecolor=colores['prediccion']))
    
    # ==================== FLUJOS PRINCIPALES ====================
    # Flechas entre capas
    ax.arrow(1.5, 10, 1.8, 0, head_width=0.2, head_length=0.2, fc='black', ec='black')
    ax.text(2.5, 10.3, '5 ‚Üí 32 conexiones', ha='center', va='center', fontsize=9)
    
    ax.arrow(4.5, 8, 1.8, 0, head_width=0.2, head_length=0.2, fc='black', ec='black')
    ax.text(5.5, 8.3, '32 ‚Üí 16 conexiones', ha='center', va='center', fontsize=9)
    
    ax.arrow(7.5, 6, 1.8, 0, head_width=0.2, head_length=0.2, fc='black', ec='black')
    ax.text(8.5, 6.3, '16 ‚Üí 1 conexi√≥n', ha='center', va='center', fontsize=9)
    
    # ==================== DETALLES T√âCNICOS ====================
    detalles = [
        "‚Ä¢ Cada neurona se conecta con TODAS las de la siguiente capa",
        "‚Ä¢ Dropout: 30% de neuronas se APAGAN aleatoriamente en entrenamiento",
        "‚Ä¢ Activaci√≥n: ReLU en capas ocultas, Sigmoid en salida",
        "‚Ä¢ Entrada: 5 caracter√≠sticas ‚Üí Salida: 1 probabilidad"
    ]
    
    for i, detalle in enumerate(detalles):
        ax.text(6, 2 - i*0.4, detalle, ha='center', va='center', fontsize=10)
    
    # ==================== LEYENDA ====================
    leyenda_simple = [
        "Blue: Input Neuron",
        "Green: Active Neuron (ReLU)", 
        "Red X: Dropout Neuron (Off)",
        "Red: Output Neuron (Sigmoid)",
        "Gray: Connections"
    ]

    for i, texto in enumerate(leyenda_simple):
        ax.text(1, 1 - i*0.3, texto, ha='left', va='center', fontsize=9)

    plt.tight_layout()
    plt.show()

# Ejecutar visualizaci√≥n 
print("ARQUITECTURA CON DROPOUT INTEGRADO")
visualizar_arquitectura_corregida()
```